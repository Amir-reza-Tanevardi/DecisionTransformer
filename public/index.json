[{"content":"Introduction This paper investigates the assumeption that we hold true for modeling reinforcement learning problems and argues how they might be wrong and actually hold us back in our path through modeling of intelligence in the reinforcement learning paradigm. The paper called these assumptions dogmas.\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. \u0026ndash;From the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus\nThe paper introduces three dogmas:\nThe Environment Spotlight Learning as Finding a Solution The Reward Hypothesis (is not actually a dogma!) The author argues the true reinforcement learning landscape is actualy like this\nIn the words of Rich Sutton:\nRL can be viewed as a microcosm of the whole AI problem.\nBut the RL today is like the following ☹️\nAnd these three dogmas are what shrink the landscape of RL.\nAnd argues we should consider getting rid of them.\nTo restore back the true RL landscape.\nBackground The author refer to the Thomas Kuhn book\nIn which he distinguishes between two phases of scientific activity\nNormal Science, which is like puzzle-solving. Revolutionary Phase, which consists of a re-imagining of the basic values, methods, and commitments of the science that Kuhn collectively calls a \u0026ldquo;paradigm\u0026rdquo;. The following is an example of previous paradigm shift in science.\nAnd the author wants to investigate the paradigm shift needed in RL.\nDogma One: The Environment Spotlight The first dogma we call the environment spotlight, which refers to our collective focus on modeling environments and environment-centric concepts rather than agents.\nWhat do we mean when we say that we focus on environments? We suggest that it is easy to answer only one of the following two questions:\nWhat is at least one canonical mathematical model of an environment in RL?\nMDP and its variants! And we define everything in terms of it. By embracing the MDP, we are allowed to import a variety of fundamental results and algorithms that define much of our primary research objectives and pathways. For example, we know every MDP has at least one deterministic, optimal, stationary policy, and that dynamic programming can be used to identify this policy. What is at least one canonical mathematical model of an agent in RL?\nIn contrast, this question has no clear answer! The author suggests it is important to define, model, and analyse agents in addition to environments. We should build toward a canonical mathematical model of an agent that can open us to the possibility of discovering general laws governing agents (if they exist).\nDogma Two: Learning as Finding a Solution The second dogma is embedded in the way we treat the concept of learning. We tend to view learning as a finite process involving the search for—and eventual discovery of—a solution to a given task.\nWe tend to implicitly assume that the learning agents we design will eventually find a solution to the task at hand, at which point learning can cease. Such agents can be understood as searching through a space of representable functions that captures the possible action-selection strategies available to an agent, similar to the Problem Space Hypothesis, and, critically, this space contains at least one function—such as the optimal policy of an MDP—that is of sufficient quality to consider the task of interested solved. Often, we are then interested in designing learning agents that are guaranteed to converge to such an endpoint, at which point the agent can stop its search (and thus, stop its learning).\nThe author suggests to embrace the view that learning can also be treated as adaptation. As a consequence, our focus will drift away from optimality and toward a version of the RL problem in which agents continually improve, rather than focus on agents that are trying to solve a specific problem.\nWhen we move away from optimality,\nHow do we think about evaluation? How, precisely, can we define this form of learning, and differentiate it from others? What are the basic algorithmic building blocks that carry out this form of learning, and how are they different from the algorithms we use today? Do our standard analysis tools such as regret and sample complexity still apply? These questions are important, and require reorienting around this alternate view of learning.\nThe author introduces the following book\nAnd the idea of Finite and Infinite Games by the following quote from the book\nThere are at least two kinds of games, One could be called finite; the other infinite. A finite game is played for the purpose of winning, an infinite game for the purpose of continuing the play.\nAnd argues alignment is an infinite game.\nDogma Three: The Reward Hypothesis The third dogma is the reward hypothesis, which states \u0026ldquo;All of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).\u0026rdquo;\nThe author suggests that the reward hypothesis is not actually a dogma! However, as we continue our quest for the design of intelligent agents, it is important to recognize the nuance in the hypothesis.\nThe reward hypothesis basically says,\nIn recent analysis by [2] fully characterizes the implicit conditions required for the hypothesis to be true. These conditions come in two forms. First, [2] provide a pair of interpretative assumptions that clarify what it would mean for the reward hypothesis to be true or false—roughly, these amount to saying two things (brwon doors).\nFirst, that \u0026ldquo;goals and purposes\u0026rdquo; can be understood in terms of a preference relation on possible outcomes. Second, that a reward function captures these preferences if the ordering over agents induced by value functions matches that of the ordering induced by preference on agent outcomes. It formalizes to the following conjecture\nThen, under this interpretation, a Markov reward function exists to capture a preference relation if and only if the preference relation satisfies the four von Neumann-Morgenstern axioms, and a fifth Bowling et al. call $\\gamma$-Temporal Indifference.\nAxiom 1: Completeness \u0026gt; You have a preference between every outcome pair.\nYou can always compare any two choices. Axiom 2: Transitivity \u0026gt; No preference cycles.\nIf you like chocolate more than vanilla, and vanilla more than strawberry, you must like chocolate more than strawberry. Axiom 3: Independence \u0026gt; Independent alternatives can\u0026rsquo;t change your preference.\nIf you like pizza more than salad, and you have to choose between a lottery of pizza or ice cream and a lottery of salad or ice cream, you should still prefer the pizza lottery over the salad lottery. Axiom 4: Continuity \u0026gt; There is always a break even chance.\nImagine you like a 100 dollar bill more than a 50 dollar bill, and a 50 dollar bill more than a 1 dollar bill. There should be a scenario where getting a chance at 100 dollar and 1 dollar, with certain probabilities, is equally good as getting the 50 dollar for sure. These 4 axioms are called the von Neumann-Morgenstern axioms.\nAxiom 5: Temporal $\\boldsymbol{\\gamma}$-Indifference \u0026gt; Discounting is consistent throughout time. Temporal $\\gamma$-indifference says that if you are indifferent between receiving a reward at time $t$ and receiving the same reward at time $t+1$, then your preference should not change if we move both time points by the same amount. For instance, if you don\u0026rsquo;t care whether you get a candy today or tomorrow, then you should also not care whether you get the candy next week or the week after. By considering the above axioms, the reward conjecture turns into reward theorem,\nWe also have to take intro account people don\u0026rsquo;t always satisfy all the axioms and humand preference could be different.\nIt is important that we are aware of the implicit restrictions we are placing on the viable goals and purposes under consideration when we represent a goal or purpose through a reward signal. We should become familiar with the requirements imposed by the five axioms, and be aware of what specifically we might be giving up when we choose to write down a reward function.\nSee Also David Abel Presentation @ ICML 2023 David Abel Personal Website Mark Ho Personal Website Anna Harutyunyan Personal Website References [1] Abel, David, Mark K. Ho, and Anna Harutyunyan. \u0026ldquo;Three Dogmas of Reinforcement Learning.\u0026rdquo; arXiv preprint arXiv:2407.10583 (2024).\n[2] Bowling, Michael, et al. \u0026ldquo;Settling the reward hypothesis.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n","permalink":"http://localhost:1313/posts/three-dogmas-of-reinforcement-learning/","summary":"Introduction This paper investigates the assumeption that we hold true for modeling reinforcement learning problems and argues how they might be wrong and actually hold us back in our path through modeling of intelligence in the reinforcement learning paradigm. The paper called these assumptions dogmas.\nDogma: A fixed, especially religious, belief or set of beliefs that people are expected to accept without any doubts. \u0026ndash;From the Cambridge Advanced Learner\u0026rsquo;s Dictionary \u0026amp; Thesaurus","title":"Three Dogmas of Reinforcement Learning"},{"content":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again. This method combined with an invariance based approach achieves new state-of-the-art results on ProcGen.\nBackground I know it\u0026rsquo;s a lot to take in! You may be wondering:\nWhat is reinforcement learning? 🥺 What generalization means for RL? 🥲 What is zero-shot generalization? 🥹 What are max reward and max entropy agents?! ☹️ What is an ensamble of them?!! 😟 What is an invariance based approach? 😓 And what the heck is ProcGen?!!! 😠 Don\u0026rsquo;t worry! We are going to cover all of that and more! And you are going to fully understand this paper and finish reading this article with an smile 🙂!\nWhat is reinforcement learning? Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. Imagine yourself right now, in reinforcement learning terms, you are an agent and everything that is not you, is your environment. You perceive the world through your senses (e.g., eyes, ears, etc.) and what you perceive turns into electrical signals that your brain processes to form an understanding of your surroundings (state). Based on this understanding, you make decisions (actions) with the goal of achieving the best possible outcome for yourself (reward).\nIn a more formal sense, reinforcement learning involves the following components:\nAgent: The learner or decision maker (e.g., you). Environment: Everything the agent interacts with (e.g., the world around you). State ($s$): A representation of the current situation of the agent within the environment (e.g., what you see, hear, and feel at any given moment). Actions ($a$): The set of all possible moves the agent can make (e.g., moving your hand, walking, speaking). Reward ($r$): The feedback received from the environment in response to the agent’s action (e.g., pleasure from eating food, pain from touching a hot surface). Policy ($\\pi$): A strategy used by the agent to decide which actions to take based on the current state (e.g., your habits and decision-making processes). Value Function ($V$): A function that estimates the expected cumulative reward of being in a certain state and following a particular policy (e.g., your prediction of future happiness based on current actions). The objective of the agent is to develop a policy that maximizes the total cumulative reward over time. This is typically achieved through a process of exploration (trying out new actions to discover their effects) and exploitation (using known actions that yield high rewards).\nIn mathematical terms, the goal is to find a policy $\\pi$ that maximizes the expected return $G_t$, which is the cumulative sum of discounted rewards:\n$$ G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots $$\nwhere $\\gamma$ (0 ≤ $\\gamma$ \u0026lt; 1) is the discount factor that determines the importance of future rewards.\nFor a more thorough and in depth explanation of reinforcement learning please refer to Lilian Weng excelent blog post A (Long) Peek into Reinforcement Learning.\nWhat generalization means for RL? In reinforcement learning, generalization involves an agent\u0026rsquo;s ability to apply learned policies or value functions to new states or environments that it has not encountered during training. This is essential because it is often impractical or impossible to train an agent on every possible state it might encounter.\nThere are bunch of methods that researchers have used to tackle the problem of generalization in reinforcement learning that are summarized in the following diagram.\nThis paper focuses on RL-Specific solutions and specifically exploration technique to tackle the problem of generalization. If you\u0026rsquo;re interested to learn more about the generalization problem in reinforcement learning please refer to reference [3].\nWhat is zero-shot generalization? Zero-shot generalization in RL refers to the ability of an agent to perform well in entirely new environments or tasks without any prior specific training or fine-tuning on those environments or tasks. This is a significant challenge because it requires the agent to leverage its learned representations and policies in a highly flexible and adaptive manner.\nIn order to define the objective of zero-shot generalization we first have to define what MDPs and POMDPs are.\nMarkov Decision Process (MDP):\nMDP is a mathematical framework used to describe an environment in reinforcement learning where the outcome is partly random and partly under the control of a decision-maker (agent). A MDP is a tuple $M = (S, A, P_{init}, P, r, \\gamma)$ where,\nStates ($S \\in \\mathbb{R}^{|S|}$): A finite set of states that describe all possible situations in which the agent can be. Actions ($A \\in \\mathbb{R}^{|A|}$): A finite set of actions available to the agent. Initial State Distribution ($P_{init}$): A distribution of starting state $(s_0 \\sim P_{init})$. Transition Probability ($P$): A function $P(s_{t+1}, s_t, a_t)$ representing the probability of transitioning from state $s_t$ to state $s_{t+1}$ after taking action $a_t$ $(s_{t+1} \\sim P(.|s_t,a_t))$. Reward ($r: S \\times A \\rightarrow \\mathbb{R}$): A function $r(s_t, a_t)$ representing the immediate reward $r_t$ received after transitioning from state $s_t$ due to action $a_t$ $(r_t = r(s_t, a_t))$. Discount Factor ($\\gamma$): $(0 \\leq \\gamma \u0026lt; 1)$ is a constant that determines the importance of future rewards. Partially Observable Markov Decision Process (POMDP):\nPOMDP extends MDPs to situations where the agent does not have complete information about the current state. Instead, the agent must make decisions based on partial observations. A POMDP is a tuple $M = (S, A, O, P_{init}, P, \\Sigma, r, \\gamma)$ where other that above definitions for MDP,\nObservation Space ($O$): A finite set of observations the agent can receive about the state. Observation Function ($\\Sigma$): A function that given current state $s_t$ and current action $a_t$ gives us the current observation $o_t$ $(o_t = \\Sigma(s_t, a_t) \\in O)$. If we set $O = S$ and $\\Sigma(s,a) = s$, the POMDP turns into regular MDP.\nLet the history at time $t$ be,\n$$ h_t = \\{ o_0, a_0, r_0, o_1, a_1, r_1, \\dots, o_t \\} $$\nThe agent’s next action is outlined by a policy $\\pi$, which is a stochastic mapping from the history to an action probability,\n$$ \\pi(a|h_t) = P(a_t=a|h_t) $$\nIn this formulation, a history-dependent policy (and not a Markov policy) is required both due to partially observed states, epistemic uncertainty, and also for optimal maxEnt exploration.\nWe assume a prior distribution over POMDPs $P(M)$, defined over some space of POMDPs. For a given POMDP, an optimal policy maximizes the expected discounted return,\n$$ \\mathbb{E}_{\\pi,M} \\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] $$\nWhere the expectation is taken over the policy $\\pi(h_t)$, and the state transition probability $s_t \\sim P$ of POMDP $M$.\nOur generalization objective is to maximize the discounted cumulative reward taken in expectation over the POMDP prior,\n$$ \\mathcal{R}_ {pop}(\\pi) = \\mathbb{E}_ {M \\sim P(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} $$\nSeeking a policy that performs well in expectation over any POMDP from the prior corresponds to zero-shot generalization.\nAnd as you may have guessed we don\u0026rsquo;t have access to true prior distribution of POMDPs so we have to estimate it with $N$ training POMDPs $M_1, M_2, \\dots, M_N$ sampled from the true prior distribution $P(M)$. So we are going to maximize empirical discounted cumulative reward,\n$$ \\begin{align*} \\mathcal{R}_ {emp}(\\pi) \u0026amp;= \\frac{1}{N} \\sum _ {i=1}^{N} \\mathbb{E} _ {\\pi,M_i} \\left[ \\sum _ {t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right] \\\\ \u0026amp;= \\mathbb{E}_ {M \\sim \\hat{P}(M)} {\\left[ \\mathbb{E}_{\\pi,M} {\\left[ \\sum _{t=0}^{\\infty} \\gamma^t r(s_t,a_t) \\right]} \\right]} \\end{align*} $$\nWhere the empirical POMDP distribution $\\hat{P}(M)$ can be different from the true distribution, i.e. $\\hat{P}(M) \\neq P(M)$. In general, a policy that optimizes the empirical reward may perform poorly on the population reward and this is known as overfitting in statistical learning theory.\nWhat are max reward and max entropy agents?! As we have seen, the goal of agents in reinforcement learning is to find a policy $\\pi$ that maximizes the expected discounted return by focusing on actions that lead to the greatest immediate or future rewards. We call these common RL agents \u0026ldquo;max reward agents\u0026rdquo; in this paper. On the other hand, \u0026ldquo;max entropy agents\u0026rdquo; aim to maximize the entropy of the policy for visiting different states. Maximizing entropy encourages the agent to explore a wider range of actions that lead the agent to visit new states even when they don\u0026rsquo;t contribute any reward.\nThis type of agent will help us to make decisions when we have epistemic uncertainty about what to do at test time. Epistemic uncertainty basically means the uncertainty that we have because of our lack of knowledge and can be improved by gathering more information about the situation.\nThe insight of the authors of this paper is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore they expect such learned behavior to generalize well.\nWhat is an ensamble of them?!! Ensembling in reinforcement learning involves combining the policies of multiple individual agents to make more accurate and robust decisions. The idea is that by aggregating the outputs of different agents, we can leverage their diverse perspectives and expertise to improve overall performance.\nThis paper uses an ensamble of max reward agents to help the agent decide on the overal epistemic uncetainty that we have at test time.\nWhat is an invariance based approach? Invariance based algorithms in reinforcement learning focus on developing policies that are robust to changes and variations in the environment. These algorithms aim to identify and leverage invariant features or patterns that remain consistent across different environments or tasks. The goal is to ensure that the learned policy performs well not just in the training environment but also in new, unseen environments.\nThis paper uses IDAAC algorithm which is a special kind of DAAC algorithm as its invariance based algorithm.\nDAAC (Decoupled Advantage Actor-Critic) uses two separate networks, one for learning the policy and advantage, and one for learning the value. The value estimates are used to compute the advantage targets.\nIDAAC (Invariant Decoupled Advantage Actor-Critic) adds an additional regularizer to the DAAC policy encoder to ensure that it does not contain episode-specific information. The encoder is trained adversarially with a discriminator so that it cannot classify which observation from a given pair $(s_i, s_j)$ was first in a trajectory.\nFor more information about IDAAC algorithm please refer to reference [4].\nWhat the heck is ProcGen?!!! Procgen is a benchmark suite for evaluating the generalization capabilities of reinforcement learning agents. It was developed by OpenAI and consists of a collection of procedurally generated environments that vary in terms of visual appearance, dynamics, and difficulty. The goal of Procgen is to provide a standardized and challenging set of environments that can be used to assess the ability of RL algorithms to generalize to unseen scenarios.\nIf you are new to reinforcement learning, I know these explanations are a lot to take in! If you find yourself lost, I recommend to check out the following courses at your leisure:\nDeep Reinforcement Learning (by Hugging Face 🤗) Reinforcement Learning (by Mutual Information) Introduction to Reinforcement Learning (by David Silver) Reinforcement Learning (by Michael Littman \u0026amp; Charles Isbell) Reinforcement Learning (by Emma Brunskill) Deep Reinforcement Learning Bootcamp 2017 Foundations of Deep RL (by Pieter Abbeel) Deep Reinforcement Learning \u0026amp; Control (by Katerina Fragkiadaki) Deep Reinforcement Learning (by Sergey Levine) After reviewing all this we can focus on the rest of the paper!\nHidden Maze Experiment One of the key observation of the authors of this paper is that invariance is not enough for zero-shot generalization of reinforcemen learning algorithm. They designed the hidden maze experiment too demonstrate that. Imagine Maze, but with the walls and goal hidden in the observation. Arguably, this is the most task-invariant observation possible, such that a solution can still be obtained in a reasonable time.\nAn agent with memory can be trained to optimally solve all training tasks: figuring out wall positions by trying to move ahead and observing the resulting motion, and identifying based on its movement history in which training maze it is currently in. Obviously, such a strategy will not generalize to test mazes. Performance in Maze, where the strategy for solving any particular training task must be indicative of that task, has largely not improved by methods based on invariance\nThe following figure shows PPO performance on the hidden maze task, indicating severe overfitting.\nAs described by [5], an agent can overcome test-time errors in its policy by treating the perfect policy as an unobserved variable. The resulting decision making problem, termed the epistemic POMDP, may require some exploration at test time to resolve uncertainty. The article further proposed the LEEP algorithm based on this principle, which trains an ensemble of agents and essentially chooses randomly between the members when the ensemble does not agree, and was the first method to present substantial generalization improvement on Maze.\nIn this paper authors extend this idea and asked, How to improve exploration at test time?, and their approach is based on a novel discovery, when they train an agent to explore the training domains using a maximum entropy objective, they observe that the learned exploration behavior generalizes surprisingly well (much better than the generalization attained when training the agent to maximize reward).\nIn the following section we gonna dig deep into internals of maximum entropy policy.\nMaxEnt Policy For simplicity the authors discuss this part for the MDP case. A policy $\\pi$, through its interaction with an MDP, induces a t-step state distribution over the state space $S$,\n$$ d _ {t,\\pi} (s) = p(s_t=s | \\pi) $$\nThe objective of maximum entropy exploration is given by:\n$$ \\mathcal{H}(d(.)) = -\\mathbb{E} _ {s \\sim d} \\left[ \\log{d(s)} \\right] $$\nWhere $d$ can be regarded as either,\nStationary state distribution (infinite horizon): $d _ {\\pi} = \\lim _ {t \\rightarrow \\infty} d _ {t,\\pi} (s)$ Discounted state distribution (infinite horizon): $d _ {\\gamma, \\pi} = (1-\\gamma) \\sum _ {t=0} ^ {\\infty} \\gamma^t d _ {t,\\pi} (s)$ Marginal state distribution (finite horizon): $d _ {T, \\pi} = \\frac{1}{T} \\sum _ {t=0} ^ {T} d _ {t,\\pi} (s)$ In this work they focus on the finite horizon setting and adapt the marginal state distribution $d _ {T, \\pi}$ in which $T$ equals the episode horizon $H$, so we seek to maximize the objective:\n$$ \\begin{align*} \\mathcal{R} _ {\\mathcal{H}} (\\pi) \u0026amp;= \\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(d _ {H,\\pi}) \\right] \\\\ \u0026amp;=\\mathbb{E} _ {M \\sim \\hat{P}(M)} \\left[ \\mathcal{H}(\\frac{1}{H} \\sum _ {t=0} ^ {H} d _ {t,\\pi} (s)) \\right] \\end{align*} $$\nwhich yields a policy that \u0026ldquo;equally\u0026rdquo; visits all states during the episode.\nTo maximize this objective we can estimating the density of the agent\u0026rsquo;s state visitation distribution, but in this paper the authors adapt the non-parametric entropy estimation approach; we estimate the entropy using the particle based k-nearest neighbor (k-NN estimator).\nTo estimate the distribution $d _ {H,\\pi}$ over the states $S$, we consider each trajectory as $H$ samples of states $\\{ s_t \\} _ {t=1} ^ {H}$ and take $s _ t ^ {\\text{k-NN}}$ to be the k-NN of the state $s_t$ within the trajectory,\n$$ \\hat{ \\mathcal{H} } ^ {k,H} (d _ {H,\\pi}) \\approx \\frac{1}{H} \\sum _ {t=1} ^ {H} \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nIn which we define intrinsic reward function as,\n$$ r_I (s_t) \\coloneqq \\log ( \\parallel s_t - s_t^{\\text{k-NN}} \\parallel _ 2) $$\nThis formulation enables us to deploy any RL algorithm to approximately optimize objective.\nSpecifically, in this work we use the policy gradient algorithm PPOو where at every time step $t$, the state $s_t^{\\text{k-NN}}$ is chosen from previous states $\\{ s_t \\} _ {t=1} ^ {t-1}$ of the same episode. To improve computational efficiency, instead of taking the full observation as the state (64 x 64 RGB image), we sub-sample the observation by applying average pooling of 3 x 3 to produce an image of size 21 x 21.\nWe found that agents trained for maximum entropy exploration exhibit a smaller generalization gap compared with the standard approach of training solely with extrinsic reward. The policies are equipped with a memory unit (GRU) to allow learning of deterministic policies that maximize the entropy.\nIn all three environments, we demonstrate a small generalization gap, as test performance on unseen levels closely follows the performance achieved during training.\nIn addition, we verify that the train results are near optimal by comparing with a hand designed approximately optimal exploration policy. For example, on Maze we use the well known maze exploring strategy wall follower, also known as the left/right-hand rule.\nExpGen Algorithm Our main insight is that, given the generalization property of the entropy maximization policy established above, an agent can apply this behavior in a test MDP and expect effective exploration at test time. We pair this insight with the epistemic POMDP idea, and propose to play the exploration policy when the agent faces epistemic uncertainty, hopefully driving the agent to a different state where the reward-seeking policy is more certain.\nOur framework comprises two parts: an entropy maximizing network and an ensemble of networks that maximize an extrinsic reward to evaluate epistemic uncertainty. The first step entails training a network equipped with a memory unit to obtain a maxEnt policy $\\pi_H$ that maximizes entropy. Next, we train an ensemble of memory-less policy networks $\\{ \\pi _ r ^ j \\} _ {j=1} ^ {m} $ to maximize extrinsic reward.\nHere is the ExpGen algorithm,\nWe consider domains with a finite action space, and say that the policy $\\pi _ r ^ i$ is certain at state $s$ if its action $a_i \\sim \\pi _ r ^ i (a|s)$ is in consensus with the ensemble: $a_i = a_j$ for the majority of $k$ out of $m$, where $k$ is a hyperparameter of our algorithm.\nSwitching between two policies may result in a case where the agent repeatedly toggles between two states (if, say, the maxEnt policy takes the agent from state $s_1$ to a state $s_2$, where the ensemble agrees on an action that again moves to state $s_1$.). To avoid such “meta-stable” behavior, we randomly choose the number of maxEnt steps $n_{\\pi_{\\mathcal{H}}}$ from a Geometric distribution, $n_{\\pi_{\\mathcal{H}}} \\sim Geom(\\alpha)$.\nExperiments Our experimental setup follows ProcGen\u0026rsquo;s easy configuration, wherein agents are trained on 200 levels for 25M steps and subsequently tested on random levels. All agents are implemented using the IMPALA (Importance Weighted Actor-Learner Architectures) convolutional architecture, and trained using PPO or IDAAC. For the maximum entropy agent $\\pi_H$ we incorporate a single GRU at the final embedding of the IMPALA convolutional architecture. For all games, we use the same parameter $\\alpha=0.5$ of the Geometric distribution and form an ensemble of 10 networks.\nFollowing figure is comparison across all ProcGen games, with 95% bootstrap CIs highlighted in color. Score distributions of ExpGen, PPO, PLR, UCB-DrAC, PPG and IDAAC.\nFollowing figure shows in each row, the probability of algorithm X outperforming algorithm Y. The comparison illustrates the superiority of ExpGen over the leading contender IDAAC with probability 0.6, as well as over other methods with even higher probability.\nSee Also PyTorch implementation of ExpGen @ GitHub Ev Zisselman Presentation @ NeurIPS 2023 ExpGen Rebuttal Process @ OpenReview ExpGen Poster for NeurIPS 2023 References [1] Zisselman, Ev, et al. \u0026ldquo;Explore to generalize in zero-shot rl.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement learning: An introduction.\u0026rdquo; MIT press, (2020).\n[3] Kirk, Robert, et al. \u0026ldquo;A survey of zero-shot generalisation in deep reinforcement learning.\u0026rdquo; Journal of Artificial Intelligence Research 76 (2023): 201-264.\n[4] Raileanu, Roberta, and Rob Fergus. \u0026ldquo;Decoupling value and policy for generalization in reinforcement learning.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021.\n[5] Ghosh, Dibya, et al. \u0026ldquo;Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability.\u0026rdquo; Advances in neural information processing systems 34 (2021): 25502-25515.\n[6] Espeholt, Lasse, et al. \u0026ldquo;Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.\u0026rdquo; International conference on machine learning. PMLR, 2018.\n","permalink":"http://localhost:1313/posts/explore-to-generalize-in-zero-shot-rl/","summary":"Introduction This paper studies the problem of zero-shot generalization in reinforcement learning and introduces an algorithm that trains an ensamble of maximum reward seeking agents and an maximum entropy agent. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions with the help of maximum entropy agent and drive us to a novel part of the state space, where the ensemble may potentially agree again.","title":"ExpGen: Explore to Generalize in Zero-Shot RL"},{"content":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ❤️.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring 🚀!\n","permalink":"http://localhost:1313/us/","summary":"Welcome to our blog! We are a team of enthusiastic professors and students from Sharif University of Technology who are eager to share our insights and passion for this fascinating field. Our aim is to share our knowledge and excitement about this cutting-edge field with you ❤️.\nProfessors Professor Mohammad Hossein Rohban Professor Ehsaneddin Asgari Students Arash Alikhani Alireza Nobakht Labs RIML Lab NLP \u0026amp; DH Lab We hope you enjoy our blog and find our content both informative and inspiring 🚀!","title":""}]